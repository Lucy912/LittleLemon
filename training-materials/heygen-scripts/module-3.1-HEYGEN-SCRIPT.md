# Module 3, Video 1: What NOT to Put in AI
## HeyGen Production Script

**Video Length:** 7 minutes
**Word Count:** ~1,050 words
**Format:** Word-for-word spoken script
**Scene Structure:** 7 scenes covering security essentials

---

## SCENE 1: Hook (0:00-0:15)
**Slide:** Red warning symbols with sensitive data types

Before we go further, we need to talk about what could get you fired, sued, or fined. Here's what you absolutely must never put in AI. This is critical. Pay attention.

**[Word count: 35 words]**

---

## SCENE 2: Why This Matters (0:15-0:45)
**Slide:** Real incident examples with consequences

Here's the reality. ChatGPT, Claude, and Gemini are not private vaults. Your data trains their models in some cases. Data breaches happen. Samsung engineers pasted proprietary code into ChatGPT and the source code leaked. Lawyers cited fake cases generated by AI and got sanctioned. Healthcare providers shared patient data and violated HIPAA.

GDPR fines reach twenty million euros or four percent of global revenue. One leak can end careers, damage companies, violate laws. This isn't theoretical. This is happening right now. Good news: it's easy to avoid if you know the rules. Let me show you.

**[Word count: 115 words]**

---

## SCENE 3: The Never List (0:45-2:30)
**Slide:** 5 categories of forbidden data with examples

Never put these five things in AI. Category one: Personal Identifiable Information, or PII. Never share names, addresses, phone numbers, emails. Never share social security numbers, passport numbers, ID numbers. Never share dates of birth, medical records, financial information, photos of people, biometric data, or customer lists with contact details. Why? GDPR, CCPA, and privacy laws prohibit sharing without consent. Example violation: "Draft email to john.smith@company.com about his performance issues." You just shared employee PII.

Category two: proprietary and confidential information. Never share source code, algorithms, trade secrets. Never share unreleased product plans, pricing strategies, financial data before public disclosure, customer contracts, NDAs, legal agreements, or internal processes that create competitive advantage. Why? Competitors could access it. NDA violations. Loss of competitive edge. Example violation: "Review this code for our authentication system." You just exposed security architecture.

Category three: credentials and access information. Never share passwords, API keys, tokens, database connection strings, SSH keys, certificates, login credentials of any kind, or security questions and answers. Why? Direct path to breach. Immediate security risk. Example violation: "Help me debug this code" and you paste an API key. Your API key is now compromised.

Category four: sensitive business data. Never share pending deals, M&A information, unannounced layoffs, reorganizations, legal disputes, investigations, salary information, compensation data, or performance reviews with names. Why? Insider trading implications. Employee relations issues. Legal liability. Example violation: "Draft communication about upcoming twenty-percent layoff." Leak risk destroys trust and creates legal issues.

Category five: anything you don't have rights to share. Never share client data unless approved in contract. Never share partner information under NDA, third-party copyrighted material, data from other companies, or personal data of colleagues without consent. Why? Contract violations. Copyright infringement. Trust destruction. Example violation: "Analyze this customer data from ClientCo." That's an NDA violation and contract breach.

**[Word count: 335 words]**

---

## SCENE 4: The Anonymization Strategy (2:30-3:45)
**Slide:** Before/after examples showing anonymization

You can use AI for sensitive topics if you anonymize properly. Here's the three-step anonymization process.

Step one: remove all identifiers. Replace names with Company A, Person One, Client X. Remove locations. Say "a major European city," not "Amsterdam." Remove dates. Say "Q4," not "November fifteen." Remove specific numbers. Say "approximately fifty thousand euros," not "forty-seven thousand three hundred twenty-six euros." Remove unique details that could identify.

Step two: generalize context. Say "a B2B SaaS company," not "a project management tool." Say "a team member," not "senior engineer on payments team." Say "a client in finance," not "Deutsche Bank."

Step three: test the anonymization. Ask: could someone identify the person or company from this? If yes, anonymize more. If no, you're safe to proceed.

Example: performance review feedback. Wrong and identifiable: "Help me write a performance review for Sarah Johnson, senior developer on the payments team at Amsterdam office. She's been late to meetings and her last three PRs had bugs." Right and anonymized: "Help me write a performance review for a senior developer. Areas to address: punctuality in team meetings could improve, recent code submissions needed additional review cycles. How do I frame this constructively while being clear about expectations?"

What changed? Name removed. Specific role and team generalized. Location removed. Context preserved so feedback is still useful. No way to identify the actual person. You got the help you needed without compromising privacy.

**[Word count: 280 words]**

---

## SCENE 5: Safe Alternatives (3:45-5:00)
**Slide:** 5 alternatives with icons and when to use each

"But I need help with sensitive work. What do I do?" Here are five safe alternatives.

Alternative one: use enterprise AI with data controls. ChatGPT Enterprise where data is not used for training. Claude Enterprise with data privacy guarantees. Azure OpenAI which is your private instance. Google Vertex AI in a controlled environment. Cost is higher, but it protects sensitive data. When to use: your company handles regulated data.

Alternative two: local or on-premise AI. Self-hosted LLMs like Llama or Mistral on your servers. No data leaves your infrastructure. Cost requires technical setup. When to use: maximum security needed for healthcare, finance, or defense.

Alternative three: anonymization as I showed above. Remove PII, keep structure. Generalize specifics. Cost is free, just takes thought. When to use: need help with process or structure, not specific data.

Alternative four: work with templates and examples, not real data. Use fictional examples. "Here's a template customer contract" that's made up. Get the framework, apply to real data separately. Cost is free. When to use: learning how to do something new.

Alternative five: don't use AI for this task. Some things shouldn't involve AI. Consult a lawyer, not AI, for legal questions. Consult a security expert, not AI, for security architecture. Consult a doctor, not AI, for medical decisions. When to use: stakes are too high and expertise is required.

**[Word count: 260 words]**

---

## SCENE 6: Common Mistakes (5:00-6:15)
**Slide:** Common mistakes with corrections

Common mistakes and how to catch them. Mistake one: "I'll just use it once." Wrong thinking: one time won't matter. Reality: that's when leaks happen. Fix: treat every prompt as potentially public.

Mistake two: "My data isn't that interesting." Wrong thinking: no one cares about my work. Reality: aggregated data is valuable and regulators don't care if it's interesting. Fix: follow rules regardless of perceived importance.

Mistake three: "I trust this AI tool." Wrong thinking: the AI company won't misuse data. Reality: even trusted companies have breaches, change policies, and face government data requests. Fix: assume any data you share could become public.

Mistake four: "I'll delete it afterward." Wrong thinking: deleting my prompt removes it from their systems. Reality: data may already be processed, stored, used for training. Fix: never share what you wouldn't want public.

Mistake five: "Everyone else is doing it." Wrong thinking: if others share sensitive data, it must be okay. Reality: they're taking the same risk and one incident creates consequences for everyone. Fix: be the responsible one.

**[Word count: 195 words]**

---

## SCENE 7: Recap & Download (6:15-7:00)
**Slide:** Security checklist preview and key takeaways

Let's recap. Never put five things in AI: personal identifiable information, proprietary and confidential information, credentials and access information, sensitive business data, or anything you don't have rights to share.

If you need help with sensitive topics, use the three-step anonymization process. Remove identifiers, generalize context, test the anonymization. Or use safe alternatives: enterprise AI, local AI, anonymization, templates, or don't use AI for that task.

Download the AI Security Checklist. One page with the Never List, anonymization steps, and safe alternatives. Keep it visible. Share it with your team.

Next video: AI bias and fact-checking. How to verify AI outputs before you trust them. Comment: have you ever accidentally shared something sensitive in AI? Be honest.

Thanks for watching. Stay safe. See you in the next video.

**[Word count: 155 words]**

---

## SCRIPT SUMMARY

**Total Word Count:** 1,075 words
**Target:** ~1,050 words for 7-minute video (150 WPM)
**Actual Duration:** ~7:10 minutes at 150 WPM
**Pacing:** ~150 words per minute
**Scenes:** 7 scenes covering security essentials

---

## SLIDE REQUIREMENTS FOR HEYGEN

**Scene 1:** Red warning symbols overlaid on types of sensitive data
**Scene 2:** Real incident timeline with consequences shown
**Scene 3:** 5-column grid showing Never List categories with examples
**Scene 4:** Before/after comparison showing anonymization transformation
**Scene 5:** 5 alternatives presented as decision tree or flowchart
**Scene 6:** Common mistakes with strikethrough and corrections
**Scene 7:** Security checklist preview + download CTA

**Total Slides Needed:** 7 slides

---

## PRODUCTION NOTES FOR HEYGEN

- **Avatar Position:** Lower right corner
- **Script Pacing:** Serious, clear, authoritative (~150 WPM)
- **Pauses:** 1-2 second pauses after each "Never" category
- **Emphasis:** Bold these phrases:
  - "Never put in AI"
  - All five categories of the Never List
  - "Anonymize"
  - "Real incidents"
  - "GDPR fines"
  - "You are responsible"
- **Voice Tone:** Serious instructor - protective, clear, no-nonsense
- **Visual Style:** Security/warning theme - red accents for dangers, green for safe practices
- **Color Coding:** Red for forbidden, yellow for caution, green for safe alternatives

---

## KEY MESSAGING

**Core Messages:**
1. **Real consequences:** This isn't theoretical - people have been fired, fined, sued
2. **Simple rules:** Easy to follow once you know them
3. **You can still use AI:** Anonymization and alternatives make it possible
4. **Responsibility:** You own the outcomes of what you share

**Tone Balance:**
- Serious without being preach y
- Protective without being scary
- Clear without being condescending

---

**STATUS:** âœ… Script ready for HeyGen production
**NEXT STEP:** Create 7-slide deck with strong visual warnings and clear examples

---

Last Updated: November 2025
